# Configuration File for Tenbilac
# WARNING: ConfigParser configuration files are surprisingly weird for newbies:
# - Comment only on separate lines
# - Leave a field blank AND REMOVE THE ":" to specify "None"
# - But the latter only works if a string was expected, not for floats, ints, ...

[setup]

# Pick a name for your training run (e.g. net55). 
# Leave it blank and the filename of the present config file will be used instead -- this is the recommended use.
name: 

# You can provide a longer description here (this would be saved into the workdir, potentially useful).
description:

# Path to a workdir where networks and results will be or are stored.
# Note that MegaLUT supercedes this setting.
workdir: ./workdir

# Should a the config be written into the workdir when starting a training (for future reference)?
copyconfig: True

# Should a minimized version of the committee be created after each training (useful to send to a friend)?
minimize: True

[norm]

# If normers already exist in the workdir, should we reuse them (instead of
# overwriting them with new ones based on the current training data) ?
takeover: True

oninputs: True
inputtype: sa1
ontargets: True
targettype: -11

[net]

# Number of committee members
nmembers: 10

# The type of the networks (Net, MultNet)
type: MultNet
# Structure of hidden layers 
nhs: [3]

# For MultNets, specify here the intial weights for the additional nodes of the first layer.
# Set it to an empty list if you don't want the extra nodes.
mwlist: [(1, 1), (1, -1)]

# Names of the activation functions to use for different layers
actfctname: tanh
oactfctname: iden
multactfctname: iden

# Preset the network to transport the first of its inputs as output ?
startidentity: True
onlynidentity: -1

# Parameters controlling the noise added to the networks prior to training.
# Noise is only added to new blank networks, not if previously trained networks are reused.
addnoise: True
ininoisewscale: 0.1
ininoisebscale: 0.1
ininoisemultwscale: 0.0
ininoisemultbscale: 0.0


[train]

# Takeover (i.e., reuse) potential existing trainings or start the trainings from scratch ?
takeover: True

# The number of cores on which to run. Set it to 0 to run on all cores, and to 1 to avoid using multiprocessing.
ncpu: 10

# Name of the error function (mse, msb, ...).
errfctname: msb


# Regul is not yet fully implemented
useregul: False
#regulfctname
#regulweight

# Generic training parameters
valfrac: 0.5
shuffle: True
mbfrac: 1.0
mbloops: 1


# Save plots ?
autoplot: True
# Plot (and track) the average prediction errors for each case ? Could be massive !
trackbiases: True
# Write the full status and history to disk at each iteration (not needed unless the system is unstable) ?
saveeachit: False
# Extra debug-logging at each call ?
verbose: False


# Choice of training algorithm
algo: multnetbfgs

# One of the following Sections is passed as kwargs to the selected algorithm.
# The fields are read through an eval(), therefore to pass a string use repr("hello").
[algo_multnetbfgs]

nepochs: 1
maxiter_sum: 25
maxiter_mult: 25
gtol: 1e-8


[algo_bfgs]
maxiter: 20
gtol: 1e-8



[predict]

# How to combine the committee results (mean, median)
combine: mean

